{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bittf2gpucondae4f7464b69e54c128ccb31348cce2c73",
   "display_name": "Python 3.8.3 64-bit ('tf2gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASS PREDICTION ENCODER\n",
    "predictor_col = 'Class'\n",
    "result_col = 'Prediction_result'\n",
    "accepted_chars = 'abcdefghijklmnopqrstuvwxyzöäü-'\n",
    "\n",
    "# Define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(accepted_chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(accepted_chars))\n",
    "\n",
    "# Removes all non accepted characters\n",
    "def normalize(line):\n",
    "    return [c.lower() for c in line if c.lower() in accepted_chars]\n",
    "# Returns a list of n lists with n = word_vec_length\n",
    "def name_encoding(name):\n",
    "    # Encode input data to int, e.g. a->1, z->26\n",
    "    integer_encoded = [char_to_int[char] for i, char in enumerate(name) if i < word_vec_length]\n",
    "    # Start one-hot-encoding\n",
    "    onehot_encoded = list()\n",
    "    for value in integer_encoded:\n",
    "        # create a list of n zeros, where n is equal to the number of accepted characters\n",
    "        letter = [0 for _ in range(char_vec_length)]\n",
    "        letter[value] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "    # Fill up list to the max length. Lists need do have equal length to be able to convert it into an array\n",
    "    for _ in range(word_vec_length - len(name)):\n",
    "        onehot_encoded.append([0 for _ in range(char_vec_length)])\n",
    "    return onehot_encoded\n",
    "# Encode the output labels\n",
    "def lable_encoding(result_series):\n",
    "    labels = np.empty((0, 3))\n",
    "    for i in result_series:\n",
    "        if i == 'y1':\n",
    "            labels = np.append(labels, [[1,0,0]], axis=0)\n",
    "        elif i == 'y2':\n",
    "            labels = np.append(labels, [[0,1,0]], axis=0)\n",
    "        elif i == 'y3':\n",
    "            labels = np.append(labels, [[0,0,1]], axis=0)\n",
    "        else:\n",
    "            labels = np.append(labels, [[1,1,1]], axis=0)\n",
    "            print(i)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "\n",
    "# load data\n",
    "def parse(x):\n",
    "    return datetime.strptime(x, '%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "file_name = 'dataset'\n",
    "\n",
    "dataset = read_csv(\n",
    "    file_name,\n",
    "    index_col=0,\n",
    "    date_parser=parse,\n",
    "    )\n",
    "#dataset.drop('No', axis=1, inplace=True)\n",
    "\n",
    "# manually specify column names\n",
    "dataset.columns = ['X1', 'X2', 'X3', 'X4', 'Class']\n",
    "dataset.index.name = 'date'\n",
    "dataset.to_csv('Cleansing_new.csv')\n",
    "print('Prepareprocessing Complete!')\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import os \n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "df = pd.read_csv('Cleansing_new.csv')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = input features\n",
    "dfX = df[[ 'X1', 'X2', 'X3', 'X4']]\n",
    "dfY = df[[ 'Class']]\n",
    "dfX.index.name = 'No'\n",
    "dfY.index.name = 'No'\n",
    "#ENCODING CLASS\n",
    "dfY = lable_encoding(dfY['Class'])\n",
    "dfY = pd.DataFrame(dfY)\n",
    "dfY.columns =['y1', 'y2', 'y3']\n",
    "\n",
    "df2 = pd.concat([dfX, dfY], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for lstm\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    " \n",
    "\n",
    "values = df2.values\n",
    "# integer encode direction???\n",
    "encoder = LabelEncoder()\n",
    "#values[:,3] = encoder.fit_transform(values[:,3])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# specify the number of lag hours\n",
    "n_hours = 3  #lag\n",
    "n_features = 2\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_hours, 1)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[ 2,3,4,7,8,9,12,13,14,-4,-5 ]], axis=1, inplace=True) \n",
    "reframed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_train_hours = int(0.7*len(values))\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "n_obs = n_hours * n_features\n",
    "train_X, train_y = train[:, :n_obs], train[:, -3:]\n",
    "test_X, test_y = test[:, :n_obs], test[:, -3:]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "#print(len(values))\n",
    "#print(len(dfTest))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "from keras.utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import multi_gpu_model\n",
    "from tensorflow import keras\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_soft_device_placement(True)\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "with tf.device('/device:GPU:1'):\n",
    "    print('Build model...')\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(10,return_sequences=False,input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(units=3))\n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "history = model.fit(train_X, train_y, batch_size= 16 , epochs=200, validation_data=(test_X, test_y), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model save\n",
    "model.save('model_name')\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], '-')\n",
    "plt.plot(history.history['val_loss'], '-')\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train','Validate'],loc = 'best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'], '-')\n",
    "plt.plot(history.history['val_acc'], '-')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "score_train = model.evaluate(train_X, train_y, verbose=1)\n",
    "print('Train loss: {:.10f}'.format(score_train[0]) +'%')\n",
    "print('Train accuracy: {:.2f}'.format(score_train[1]*100) +'%','\\n')\n",
    "\n",
    "\n",
    "score_test = model.evaluate(test_X, test_y, verbose=1)\n",
    "print('Test loss: {:.10f}'.format(score_test[0]) +'%')\n",
    "print('Test accuracy: {:.2f}'.format(score_test[1]*100) +'%','\\n')"
   ]
  }
 ]
}